{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling origins across different cities and creating origin-destination pairs.\n",
    "\n",
    "## Prerequisites\n",
    "This repository builds on the python package OSMNx (v.2.0.1, https://osmnx.readthedocs.io/en/stable/). I recommend installing it via conda:\n",
    "```\n",
    "conda create -n ox -c conda-forge --strict-channel-priority osmnx\n",
    "```\n",
    "For sampling nodes based on city names two additional packages are required, namely geopy (v.2.3.1, https://geopy.readthedocs.io/en/stable/) and overpy (v.0.7, https://python-overpy.readthedocs.io/en/latest/)\n",
    "\n",
    "```\n",
    "pip install geopy\n",
    "pip install overpy nodes run Ubuntu Jammy 22.04 LTS.\n",
    "There is local scratch space on each node, which is shared between the jobs currently running. Connected to Kebnekaise is also our parallel file system Ransarn (where your project storage is located), which provide quick access to files regardless of which node they run on. For more information about the different file systems that are available on our systems, read the Filesystems and Storage page.\n",
    "```\n",
    "\n",
    "For visualizing routes and geometry on maps I use the folium package (v.0.19.4, https://python-visualization.github.io/folium/latest/) that is included in the OSMNx package, but for creating static images of these visualizations the Selenium package is required (v.4.28.0, https://www.selenium.dev/documentation/)\n",
    "\n",
    "```\n",
    "pip install selenium\n",
    "```\n",
    "\n",
    "## This exampleCities are used as the basis to find random samples of intersections. The region and country names are nice to have, but they are not necessary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_size = 3\n",
    "min_distance = 3\n",
    "random_seed = 3\n",
    "network_type = 'drive'\n",
    "point_distance_size = 10000\n",
    "experiment_name = \"2025-03-A\"\n",
    "base_path=f\"/proj/nobackup/streetnetwork-alignment/{experiment_name}\"\n",
    "min_od_distance = 4750\n",
    "max_od_distance = 5250\n",
    "sample_size = 144\n",
    "import os\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "    \n",
    "parameters_file_path = os.path.join(base_path, f\"{experiment_name}_parameters.csv\")\n",
    "city_sample_nodes_path = os.path.join(base_path, f'city_sample_nodes_{experiment_name}.csv')\n",
    "local_graph_folder = os.path.join(base_path,'local_origin_graphs')\n",
    "import csv\n",
    "\n",
    "with open(parameters_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Parameter\", \"Value\"])\n",
    "    writer.writerow([\"sample_size\", sample_size])\n",
    "    writer.writerow([\"min_distance\", min_distance])\n",
    "    writer.writerow([\"random_seed\", random_seed])\n",
    "    writer.writerow([\"network_type\", network_type])\n",
    "    writer.writerow([\"point_distance_size\", point_distance_size])\n",
    "    writer.writerow([\"min_od_distance\", min_od_distance])\n",
    "    writer.writerow([\"max_od_distance\", max_od_distance])\n",
    "    writer.writerow([\"base_path\", base_path])\n",
    "    writer.writerow([\"city_sample_nodes_path\", city_sample_nodes_path])\n",
    "    writer.writerow([\"local_graph_folder\", local_graph_folder])\n",
    "    writer.writerow([\"base_path\", base_path])\n",
    "    writer.writerow([\"experiment_name\", experiment_name])\n",
    "\n",
    "import pandas as pd\n",
    "param = pd.read_csv(parameters_file_path)\n",
    "display(param)\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()  # Adjust based on your system's capabilities\n",
    "print(f\"Number of processes to use: {num_processes}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The workflow for analyzing the routes begins with coordinate points used as origin locations.\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"100_city_sample.csv\")\n",
    "display(df)\n",
    "\n",
    "\n",
    "# To sample nodes from the cities we call use a method from sample_nodes.py\n",
    "import node_sampling\n",
    "\n",
    "df = node_sampling.get_random_nodes_for_all_cities(df,min_distance_km=min_distance,sample_size=sample_size,random_seed=random_seed)\n",
    "\n",
    "\n",
    "display(df)\n",
    "\n",
    "import os # for file operations\n",
    "city_sample_nodes_path = os.path.join(base_path, f'city_sample_nodes_{experiment_name}.csv')\n",
    "df.to_csv(os.path.join(base_path,city_sample_nodes_path))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from origin_graph import origin_graph  # for creating the origin graph object\n",
    "import os  # for file operations\n",
    "import ast  # for parsing string to tuple\n",
    "import pandas as pd  # for reading the csv file\n",
    "import osmnx as ox  # for plotting the graph\n",
    "import matplotlib.pyplot as plt  # for plotting the graphs\n",
    "import multiprocessing\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s',filename='jupyter.log', filemode='w')\n",
    "\n",
    "\n",
    "sub_folder = \"local_origin_graphs\"\n",
    "local_graph_folder = os.path.join(base_path, sub_folder)\n",
    "if not os.path.exists(local_graph_folder):\n",
    "    os.makedirs(local_graph_folder)\n",
    "\n",
    "print(city_sample_nodes_path)\n",
    "df = pd.read_csv(city_sample_nodes_path)\n",
    "\n",
    "def process_row(row):\n",
    "    if os.path.exists(row['graph_path']):\n",
    "        #print(f\"Graph exists: {row['graph_path']}\")\n",
    "        return f\"Graph already exists {row['graph_path']}\"\n",
    "    else:\n",
    "        print(f\"---Graph missing: {row['graph_path']}---\")\n",
    "        # Apply the function asynchronously\n",
    "    try:\n",
    "        latlon_point = ast.literal_eval(row['node_latlon'])\n",
    "        og = origin_graph(origin_point=latlon_point, distance_from_point=point_distance_size,\n",
    "                          city_name=row[\"city_name\"], network_type=network_type, remove_parallel=True, simplify=True)\n",
    "    \n",
    "        og.save_graph(row['graph_path'])\n",
    "    \n",
    "        # Plot the origin graph to see if something is obviously wrong\n",
    "        ox.plot_graph(og.graph, node_color='blue', node_size=10, edge_linewidth=1, edge_color='black', bgcolor='white',\n",
    "                       save=True, filepath=os.path.join(local_graph_folder, f\"{row['city_name']}_{row['node_id']}.png\"), show=False)\n",
    "        logging.error(f\"Finished with graph: {row['graph_path']}\")\n",
    "        return f\"Finished with graph: {row['graph_path']}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"error {e} creating {row['graph_path']}\")\n",
    "        return f\"Failed with graph: {row['graph_path']} error {e}\"\n",
    "        \n",
    "num_processes = multiprocessing.cpu_count()  # Adjust based on your system's capabilities\n",
    "print(f\"Number of processes to use: {num_processes}\")\n",
    "\n",
    "# Collect results from multiprocessing\n",
    "try:\n",
    "    # Use multiprocessing Pool for parallel processing\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        # Use pool.map instead of apply_async for better control and result handling\n",
    "        results = pool.map(process_row, [row for _, row in df.iterrows()])\n",
    "    \n",
    "    # Optional: Process and log the results\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Multiprocessing error: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "df = pd.read_csv(city_sample_nodes_path)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        if os.path.exists(row['graph_path']):\n",
    "            #print(f\"Graph exists: {row['city_name']}_{row['node_id']}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(row['node_id'])\n",
    "            print(f\"---Graph missing: {row['graph_path']}---\")\n",
    "\n",
    "\n",
    "print(df[\"graph_path\"][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "local_graph_folder = os.path.join(base_path, \"local_origin_graphs\")\n",
    "df = pd.read_csv(city_sample_nodes_path)\n",
    "df['graph_path'] = df.apply(lambda row: os.path.join(local_graph_folder, f\"{row['city_name']}_{row['node_id']}.graphml\"), axis=1)\n",
    "df['random_seed'] = random_seed\n",
    "df['min_distance'] = min_distance\n",
    "df['point_distance_size'] = point_distance_size\n",
    "display(df)\n",
    "df.to_csv(city_sample_nodes_path, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import joblib\n",
    "num_processes = joblib.cpu_count()\n",
    "print(f\"Number of processes to use: {num_processes}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# The next step is to add weights to the edges of the graph.\n",
    "from origin_graph import origin_graph # for creating the origin graph object\n",
    "import pandas as pd # for reading the csv file\n",
    "import joblib # replacing multiprocessing with joblib\n",
    "\n",
    "df = pd.read_csv(city_sample_nodes_path)\n",
    "\n",
    "if 'weights_added' not in df.columns:\n",
    "    df['weights_added'] = None\n",
    "\n",
    "def add_graph_weights(row):\n",
    "    try:\n",
    "        og = origin_graph.from_graphml(graphml_path=row['graph_path'])\n",
    "        og.add_simplest_paths_from_origin()\n",
    "        og.add_weights('deviation_from_prototypical')\n",
    "        og.add_weights('node_degree')\n",
    "        og.add_weights('instruction_equivalent')\n",
    "        og.save_graph(row['graph_path'])\n",
    "        print(f\"Finished with graph: {row['city_name']} node: {row['node_id']}\")\n",
    "        return True,row['city_name'],row['node_id']\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with graph: {row['graph_path']} error {e}\")\n",
    "        return False,row['city_name'],row['node_id']\n",
    "\n",
    "# Number of processes to use\n",
    "num_processes = joblib.cpu_count()\n",
    "print(f\"Number of processes to use: {num_processes}\")\n",
    "\n",
    "# Collect results from joblib\n",
    "try:\n",
    "    # Use joblib's Parallel and delayed for parallel processing\n",
    "    results = joblib.Parallel(n_jobs=num_processes,backend='loky')(\n",
    "        joblib.delayed(add_graph_weights)(row) for _, row in df.iterrows()\n",
    "    )\n",
    "    for result in results:\n",
    "        if result[0]:  # Check if the value in results[0] is True\n",
    "            # Find the row in df where both 'city_name' and 'start_node' match\n",
    "            mask = (df['city_name'] == result[1]) & (df['start_node'] == result[2])\n",
    "            df.loc[mask, 'weights_added'] = True  # Update 'weights_added' to True\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Joblib parallel processing error: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd # for reading the csv file\n",
    "import joblib # replacing multiprocessing with joblib\n",
    "\n",
    "df = pd.read_csv(city_sample_nodes_path)\n",
    "\n",
    "local_odpair_folder = os.path.join(base_path, \"od_pair_data\")\n",
    "print(f\"odpair data will be stored at {local_odpair_folder}\")\n",
    "os.makedirs(local_odpair_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "if 'od_pairs_added' not in df.columns:\n",
    "    df['od_pairs_added'] = False\n",
    "\n",
    "def get_od_pairs(row):\n",
    "    try:\n",
    "        og = origin_graph.from_graphml(graphml_path=row['graph_path'])\n",
    "        og.create_od_pairs(min_radius=min_od_distance, max_radius=max_od_distance, sample_size=144)\n",
    "        od_pair_data = og.get_od_pair_data()\n",
    "        json_path = os.path.join(local_odpair_folder, f\"od_pair_{row['city_name']}_{row['node_id']}.json\")\n",
    "        od_pair_data.to_json(json_path, orient=\"records\", default_handler=str, indent=2)\n",
    "        print(f\"Finished finding OD_pairs for graph: {row['city_name']} node: {row['node_id']}\")\n",
    "        return True,row['city_name'],row['node_id']\n",
    "    except Exception as e:\n",
    "        print(f\"Failed finding OD_pairs for  graph: {row['graph_path']} error {e}\")\n",
    "        return False,row['city_name'],row['node_id']\n",
    "\n",
    "\n",
    "num_processes = joblib.cpu_count()\n",
    "print(f\"Number of processes to use: {num_processes}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    rows_to_process = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if not row['od_pairs_added']:\n",
    "            rows_to_process.append(row)\n",
    "\n",
    "    results = joblib.Parallel(n_jobs=num_processes, backend='loky')(\n",
    "        joblib.delayed(add_graph_weights)(row) for row in rows_to_process\n",
    "    )\n",
    "\n",
    "    for result in results:\n",
    "        if result[0]:\n",
    "            mask = (df['city_name'] == result[1]) & (df['start_node'] == result[2])\n",
    "            df.loc[mask, 'od_pairs_added'] = True\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Joblib parallel processing error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import post_processing\n",
    "import pandas as pd\n",
    "od_pair_data = pd.read_json(\"example/origin_od_pairs.json\")\n",
    "\n",
    "od_pair_data = post_processing.label_length_outliers(od_pair_data)\n",
    "od_pair_data = post_processing.label_gridlike_groups(od_pair_data)\n",
    "\n",
    "# Before normalizing the complexity, we need to remove the length outliers.\n",
    "print(f\"od-pairs before removing length outliers {len(od_pair_data)}\")\n",
    "od_pair_data = od_pair_data[od_pair_data['length_outliers'] == False]\n",
    "print(f\"od-pairs after removing length outliers {len(od_pair_data)}\")\n",
    "od_pair_data = post_processing.normalize_complexity(od_pair_data)\n",
    "print(len(od_pair_data))\n",
    "# The od-pair data contains lists and dictionaries that are not easily saved to a csv file, so we store it as a json file.\n",
    "# Still, there some columns that need to be serialized to strings such as shapely polygon objects.\n",
    "od_pair_data.to_json(\"example/origin_od_pairs.json\",orient=\"records\",default_handler=str,indent=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "od_pair_data = pd.read_json(\"example/origin_od_pairs.json\")\n",
    "\n",
    "\n",
    "od_pair_data['closest_strongest_lag'] = abs(od_pair_data['closest_strongest_lag'])\n",
    "\n",
    "od_pair_data = od_pair_data.sort_values(by=\"closest_strongest_lag\", ascending=True)\n",
    "\n",
    "\n",
    "city_counts = od_pair_data['city_name'].value_counts()\n",
    "city_counts.plot(kind='bar')\n",
    "plt.xlabel('City Name')\n",
    "plt.ylabel('Number of od-pairs')\n",
    "plt.title('Number of od-pairs in Each city')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osmnx201_venv",
   "language": "python",
   "name": "osmnx201_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
